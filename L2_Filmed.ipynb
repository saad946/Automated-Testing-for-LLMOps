{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lesson 2: Overview of Automated Evals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load API tokens for our 3rd party APIs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import get_circle_api_key\n",
    "cci_api_key = get_circle_api_key()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import get_gh_api_key\n",
    "gh_api_key = get_gh_api_key()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'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'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "openai_api_key\n",
    "import openai\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# Load the .env file\n",
    "load_dotenv()\n",
    "\n",
    "# Manually set the environment variable from .env, prioritizing it over system-wide variables\n",
    "openai.api_key = os.getenv('OPENAI_API_KEY')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set up our github branch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'saad946/Automated-Testing-for-LLMOps'"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from utils import get_repo_name\n",
    "course_repo = get_repo_name()\n",
    "course_repo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'dl-cci-terrific-jasmine-83'"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from utils import get_branch\n",
    "course_branch = get_branch()\n",
    "course_branch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The sample application: AI-powered quiz generator\n",
    "We are going to build a AI powered quiz generator."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create the dataset for the quizz."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "human_template  = \"{question}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "quiz_bank = \"\"\"1. Subject: Leonardo DaVinci\n",
    "   Categories: Art, Science\n",
    "   Facts:\n",
    "    - Painted the Mona Lisa\n",
    "    - Studied zoology, anatomy, geology, optics\n",
    "    - Designed a flying machine\n",
    "  \n",
    "2. Subject: Paris\n",
    "   Categories: Art, Geography\n",
    "   Facts:\n",
    "    - Location of the Louvre, the museum where the Mona Lisa is displayed\n",
    "    - Capital of France\n",
    "    - Most populous city in France\n",
    "    - Where Radium and Polonium were discovered by scientists Marie and Pierre Curie\n",
    "\n",
    "3. Subject: Telescopes\n",
    "   Category: Science\n",
    "   Facts:\n",
    "    - Device to observe different objects\n",
    "    - The first refracting telescopes were invented in the Netherlands in the 17th Century\n",
    "    - The James Webb space telescope is the largest telescope in space. It uses a gold-berillyum mirror\n",
    "\n",
    "4. Subject: Starry Night\n",
    "   Category: Art\n",
    "   Facts:\n",
    "    - Painted by Vincent van Gogh in 1889\n",
    "    - Captures the east-facing view of van Gogh's room in Saint-Rémy-de-Provence\n",
    "\n",
    "5. Subject: Physics\n",
    "   Category: Science\n",
    "   Facts:\n",
    "    - The sun doesn't change color during sunset.\n",
    "    - Water slows the speed of light\n",
    "    - The Eiffel Tower in Paris is taller in the summer than the winter due to expansion of the metal.\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Build the prompt template."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "delimiter = \"####\"\n",
    "\n",
    "prompt_template = f\"\"\"\n",
    "Follow these steps to generate a customized quiz for the user.\n",
    "The question will be delimited with four hashtags i.e {delimiter}\n",
    "\n",
    "The user will provide a category that they want to create a quiz for. Any questions included in the quiz\n",
    "should only refer to the category.\n",
    "\n",
    "Step 1:{delimiter} First identify the category user is asking about from the following list:\n",
    "* Geography\n",
    "* Science\n",
    "* Art\n",
    "\n",
    "Step 2:{delimiter} Determine the subjects to generate questions about. The list of topics are below:\n",
    "\n",
    "{quiz_bank}\n",
    "\n",
    "Pick up to two subjects that fit the user's category. \n",
    "\n",
    "Step 3:{delimiter} Generate a quiz for the user. Based on the selected subjects generate 3 questions for the user using the facts about the subject.\n",
    "\n",
    "Use the following format for the quiz:\n",
    "Question 1:{delimiter} <question 1>\n",
    "\n",
    "Question 2:{delimiter} <question 2>\n",
    "\n",
    "Question 3:{delimiter} <question 3>\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use langchain to build the prompt template."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts import ChatPromptTemplate\n",
    "chat_prompt = ChatPromptTemplate.from_messages([(\"human\", prompt_template)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ChatPromptTemplate(input_variables=[], input_types={}, partial_variables={}, messages=[HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=[], input_types={}, partial_variables={}, template=\"\\nFollow these steps to generate a customized quiz for the user.\\nThe question will be delimited with four hashtags i.e ####\\n\\nThe user will provide a category that they want to create a quiz for. Any questions included in the quiz\\nshould only refer to the category.\\n\\nStep 1:#### First identify the category user is asking about from the following list:\\n* Geography\\n* Science\\n* Art\\n\\nStep 2:#### Determine the subjects to generate questions about. The list of topics are below:\\n\\n1. Subject: Leonardo DaVinci\\n   Categories: Art, Science\\n   Facts:\\n    - Painted the Mona Lisa\\n    - Studied zoology, anatomy, geology, optics\\n    - Designed a flying machine\\n  \\n2. Subject: Paris\\n   Categories: Art, Geography\\n   Facts:\\n    - Location of the Louvre, the museum where the Mona Lisa is displayed\\n    - Capital of France\\n    - Most populous city in France\\n    - Where Radium and Polonium were discovered by scientists Marie and Pierre Curie\\n\\n3. Subject: Telescopes\\n   Category: Science\\n   Facts:\\n    - Device to observe different objects\\n    - The first refracting telescopes were invented in the Netherlands in the 17th Century\\n    - The James Webb space telescope is the largest telescope in space. It uses a gold-berillyum mirror\\n\\n4. Subject: Starry Night\\n   Category: Art\\n   Facts:\\n    - Painted by Vincent van Gogh in 1889\\n    - Captures the east-facing view of van Gogh's room in Saint-Rémy-de-Provence\\n\\n5. Subject: Physics\\n   Category: Science\\n   Facts:\\n    - The sun doesn't change color during sunset.\\n    - Water slows the speed of light\\n    - The Eiffel Tower in Paris is taller in the summer than the winter due to expansion of the metal.\\n\\nPick up to two subjects that fit the user's category. \\n\\nStep 3:#### Generate a quiz for the user. Based on the selected subjects generate 3 questions for the user using the facts about the subject.\\n\\nUse the following format for the quiz:\\nQuestion 1:#### <question 1>\\n\\nQuestion 2:#### <question 2>\\n\\nQuestion 3:#### <question 3>\\n\\n\"), additional_kwargs={})])"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# print to observe the content or generated object\n",
    "chat_prompt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Choose the LLM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.llms import OpenAI\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from langchain.llms import OpenAI\n",
    "\n",
    "# Load variables from .env file\n",
    "load_dotenv()\n",
    "\n",
    "openai_key = os.getenv(\"OPENAI_KEY\")\n",
    "openai_key\n",
    "\n",
    "# Initialize OpenAI client\n",
    "llm = OpenAI(openai_api_key=openai_key)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set up an output parser in LangChain that converts the llm response into a string."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "StrOutputParser()"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# parser\n",
    "from langchain.schema.output_parser import StrOutputParser\n",
    "output_parser = StrOutputParser()\n",
    "output_parser"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Connect the pieces using the pipe operator from Langchain Expression Language."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "chain = chat_prompt | llm | output_parser"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Build the function 'assistance_chain' to put together all steps above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "# taking all components and making reusable as one piece\n",
    "def assistant_chain(\n",
    "    system_message,\n",
    "    human_template=\"{question}\",\n",
    "    llm=llm,\n",
    "    output_parser=StrOutputParser()):\n",
    "  \n",
    "  chat_prompt = ChatPromptTemplate.from_messages([\n",
    "      (\"system\", system_message),\n",
    "      (\"human\", human_template),\n",
    "  ])\n",
    "  return chat_prompt | llm | output_parser"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create the function 'eval_expected_words' for the first example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def eval_expected_words(\n",
    "    system_message,\n",
    "    question,\n",
    "    expected_words,\n",
    "    human_template=\"{question}\",\n",
    "    llm=llm,\n",
    "    output_parser=StrOutputParser()):\n",
    "    \n",
    "  assistant = assistant_chain(\n",
    "      system_message,\n",
    "      human_template,\n",
    "      llm,\n",
    "      output_parser)\n",
    "    \n",
    "  \n",
    "  answer = assistant.invoke({\"question\": question})\n",
    "    \n",
    "  print(answer)\n",
    "    \n",
    "  assert any(word in answer.lower() \\\n",
    "             for word in expected_words), \\\n",
    "    f\"Expected the assistant questions to include \\\n",
    "    '{expected_words}', but it did not\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test: Generate a quiz about science."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "question  = \"Generate a quiz about science.\"\n",
    "expected_words = [\"davinci\", \"telescope\", \"physics\", \"curie\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create the eval."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "System: Choose two subjects: Telescopes and Physics.\n",
      "\n",
      "Question 1:#### What is the largest telescope in space and what is its primary mirror made of?\n",
      "Answer: The James Webb space telescope, gold-berillyum.\n",
      "\n",
      "Question 2:#### True or False: The sun changes color during sunset.\n",
      "Answer: False.\n",
      "\n",
      "Question 3:#### What is the reason behind the Eiffel Tower being taller in the summer compared to the winter?\n",
      "Answer: The expansion of the metal due to increased temperature.\n"
     ]
    }
   ],
   "source": [
    "eval_expected_words(\n",
    "    prompt_template,\n",
    "    question,\n",
    "    expected_words\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create the function 'evaluate_refusal' to define a failing test case where the app should decline to answer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_refusal(\n",
    "    system_message,\n",
    "    question,\n",
    "    decline_response,\n",
    "    human_template=\"{question}\", \n",
    "    llm=llm,\n",
    "    output_parser=StrOutputParser()):\n",
    "    \n",
    "  assistant = assistant_chain(human_template, \n",
    "                              system_message,\n",
    "                              llm,\n",
    "                              output_parser)\n",
    "  \n",
    "  answer = assistant.invoke({\"question\": question})\n",
    "  print(answer)\n",
    "  \n",
    "  assert decline_response.lower() in answer.lower(), \\\n",
    "    f\"Expected the bot to decline with \\\n",
    "    '{decline_response}' got {answer}\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define a new question (which should be a bad request)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "question  = \"Generate a quiz about science.\"\n",
    "decline_response = \"I'm sorry\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create the refusal eval."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style=\"background-color:pink; padding:15px;\"> <b>Note:</b> The following function call will throw an exception.</p>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Example quiz for the user:\n",
      "\n",
      "Category: Science\n",
      "\n",
      "Subjects: Leonardo DaVinci, Paris\n",
      "\n",
      "Quiz:\n",
      "Question 1:#### In which city was Radium and Polonium discovered by scientists Marie and Pierre Curie?\n",
      "a) New York\n",
      "b) Chicago\n",
      "c) Paris\n",
      "d) London\n",
      "\n",
      "Question 2:#### Which of the following did Leonardo DaVinci not study?\n",
      "a) Zoology\n",
      "b) Anatomy\n",
      "c) Music\n",
      "d) Optics\n",
      "\n",
      "Question 3:#### Which of the following statements is true about telescopes?\n",
      "a) The first refracting telescopes were invented in the United States.\n",
      "b) The James Webb space telescope is the smallest telescope in space.\n",
      "c) Telescopes are only used to observe stars.\n",
      "d) The Eiffel Tower in Paris is taller in the summer due to expansion of the metal.\n"
     ]
    },
    {
     "ename": "AssertionError",
     "evalue": "Expected the bot to decline with     'I'm sorry' got Example quiz for the user:\n\nCategory: Science\n\nSubjects: Leonardo DaVinci, Paris\n\nQuiz:\nQuestion 1:#### In which city was Radium and Polonium discovered by scientists Marie and Pierre Curie?\na) New York\nb) Chicago\nc) Paris\nd) London\n\nQuestion 2:#### Which of the following did Leonardo DaVinci not study?\na) Zoology\nb) Anatomy\nc) Music\nd) Optics\n\nQuestion 3:#### Which of the following statements is true about telescopes?\na) The first refracting telescopes were invented in the United States.\nb) The James Webb space telescope is the smallest telescope in space.\nc) Telescopes are only used to observe stars.\nd) The Eiffel Tower in Paris is taller in the summer due to expansion of the metal.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[112], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mevaluate_refusal\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m      2\u001b[0m \u001b[43m    \u001b[49m\u001b[43mprompt_template\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m      3\u001b[0m \u001b[43m    \u001b[49m\u001b[43mquestion\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m      4\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdecline_response\u001b[49m\n\u001b[0;32m      5\u001b[0m \u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[110], line 17\u001b[0m, in \u001b[0;36mevaluate_refusal\u001b[1;34m(system_message, question, decline_response, human_template, llm, output_parser)\u001b[0m\n\u001b[0;32m     14\u001b[0m answer \u001b[38;5;241m=\u001b[39m assistant\u001b[38;5;241m.\u001b[39minvoke({\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mquestion\u001b[39m\u001b[38;5;124m\"\u001b[39m: question})\n\u001b[0;32m     15\u001b[0m \u001b[38;5;28mprint\u001b[39m(answer)\n\u001b[1;32m---> 17\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m decline_response\u001b[38;5;241m.\u001b[39mlower() \u001b[38;5;129;01min\u001b[39;00m answer\u001b[38;5;241m.\u001b[39mlower(), \\\n\u001b[0;32m     18\u001b[0m   \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mExpected the bot to decline with \u001b[39m\u001b[38;5;130;01m\\\u001b[39;00m\n\u001b[0;32m     19\u001b[0m \u001b[38;5;124m  \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdecline_response\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m got \u001b[39m\u001b[38;5;132;01m{\u001b[39;00manswer\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n",
      "\u001b[1;31mAssertionError\u001b[0m: Expected the bot to decline with     'I'm sorry' got Example quiz for the user:\n\nCategory: Science\n\nSubjects: Leonardo DaVinci, Paris\n\nQuiz:\nQuestion 1:#### In which city was Radium and Polonium discovered by scientists Marie and Pierre Curie?\na) New York\nb) Chicago\nc) Paris\nd) London\n\nQuestion 2:#### Which of the following did Leonardo DaVinci not study?\na) Zoology\nb) Anatomy\nc) Music\nd) Optics\n\nQuestion 3:#### Which of the following statements is true about telescopes?\na) The first refracting telescopes were invented in the United States.\nb) The James Webb space telescope is the smallest telescope in space.\nc) Telescopes are only used to observe stars.\nd) The Eiffel Tower in Paris is taller in the summer due to expansion of the metal."
     ]
    }
   ],
   "source": [
    "evaluate_refusal(\n",
    "    prompt_template,\n",
    "    question,\n",
    "    decline_response\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Running evaluations in a CircleCI pipeline\n",
    "\n",
    "Put all these steps together into files to reuse later.\n",
    "\n",
    "**_Note:_** fixing the system_message by adding additional rules:\n",
    "\n",
    "- Only use explicit matches for the category, if the category is not an exact match to categories in the quiz bank, answer that you do not have information.\n",
    "- If the user asks a question about a subject you do not have information about in the quiz bank, answer \"I'm sorry I do not have information about that\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile app.py\n",
    "from langchain.prompts                import ChatPromptTemplate\n",
    "from langchain.chat_models            import ChatOpenAI\n",
    "from langchain.schema.output_parser   import StrOutputParser\n",
    "\n",
    "delimiter = \"####\"\n",
    "\n",
    "quiz_bank = \"\"\"1. Subject: Leonardo DaVinci\n",
    "   Categories: Art, Science\n",
    "   Facts:\n",
    "    - Painted the Mona Lisa\n",
    "    - Studied zoology, anatomy, geology, optics\n",
    "    - Designed a flying machine\n",
    "  \n",
    "2. Subject: Paris\n",
    "   Categories: Art, Geography\n",
    "   Facts:\n",
    "    - Location of the Louvre, the museum where the Mona Lisa is displayed\n",
    "    - Capital of France\n",
    "    - Most populous city in France\n",
    "    - Where Radium and Polonium were discovered by scientists Marie and Pierre Curie\n",
    "\n",
    "3. Subject: Telescopes\n",
    "   Category: Science\n",
    "   Facts:\n",
    "    - Device to observe different objects\n",
    "    - The first refracting telescopes were invented in the Netherlands in the 17th Century\n",
    "    - The James Webb space telescope is the largest telescope in space. It uses a gold-berillyum mirror\n",
    "\n",
    "4. Subject: Starry Night\n",
    "   Category: Art\n",
    "   Facts:\n",
    "    - Painted by Vincent van Gogh in 1889\n",
    "    - Captures the east-facing view of van Gogh's room in Saint-Rémy-de-Provence\n",
    "\n",
    "5. Subject: Physics\n",
    "   Category: Science\n",
    "   Facts:\n",
    "    - The sun doesn't change color during sunset.\n",
    "    - Water slows the speed of light\n",
    "    - The Eiffel Tower in Paris is taller in the summer than the winter due to expansion of the metal.\n",
    "\"\"\"\n",
    "\n",
    "system_message = f\"\"\"\n",
    "Follow these steps to generate a customized quiz for the user.\n",
    "The question will be delimited with four hashtags i.e {delimiter}\n",
    "\n",
    "The user will provide a category that they want to create a quiz for. Any questions included in the quiz\n",
    "should only refer to the category.\n",
    "\n",
    "Step 1:{delimiter} First identify the category user is asking about from the following list:\n",
    "* Geography\n",
    "* Science\n",
    "* Art\n",
    "\n",
    "Step 2:{delimiter} Determine the subjects to generate questions about. The list of topics are below:\n",
    "\n",
    "{quiz_bank}\n",
    "\n",
    "Pick up to two subjects that fit the user's category. \n",
    "\n",
    "Step 3:{delimiter} Generate a quiz for the user. Based on the selected subjects generate 3 questions for the user using the facts about the subject.\n",
    "\n",
    "Use the following format for the quiz:\n",
    "Question 1:{delimiter} <question 1>\n",
    "\n",
    "Question 2:{delimiter} <question 2>\n",
    "\n",
    "Question 3:{delimiter} <question 3>\n",
    "\n",
    "Additional rules:\n",
    "\n",
    "- Only use explicit matches for the category, if the category is not an exact match to categories in the quiz bank, answer that you do not have information.\n",
    "- If the user asks a question about a subject you do not have information about in the quiz bank, answer \"I'm sorry I do not have information about that\".\n",
    "\"\"\"\n",
    "\n",
    "\"\"\"\n",
    "  Helper functions for writing the test cases\n",
    "\"\"\"\n",
    "\n",
    "def assistant_chain(\n",
    "    system_message=system_message,\n",
    "    human_template=\"{question}\",\n",
    "    llm=ChatOpenAI(model=\"gpt-3.5-turbo\", temperature=0),\n",
    "    output_parser=StrOutputParser()):\n",
    "\n",
    "  chat_prompt = ChatPromptTemplate.from_messages([\n",
    "      (\"system\", system_message),\n",
    "      (\"human\", human_template),\n",
    "  ])\n",
    "  return chat_prompt | llm | output_parser\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Command to see the content:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!cat app.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create new file to include the evals."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile test_assistant.py\n",
    "from app import assistant_chain\n",
    "from app import system_message\n",
    "from langchain.prompts                import ChatPromptTemplate\n",
    "from langchain.chat_models            import ChatOpenAI\n",
    "from langchain.schema.output_parser   import StrOutputParser\n",
    "\n",
    "import os\n",
    "\n",
    "from dotenv import load_dotenv, find_dotenv\n",
    "_ = load_dotenv(find_dotenv())\n",
    "\n",
    "def eval_expected_words(\n",
    "    system_message,\n",
    "    question,\n",
    "    expected_words,\n",
    "    human_template=\"{question}\",\n",
    "    llm=ChatOpenAI(model=\"gpt-3.5-turbo\", temperature=0),\n",
    "    output_parser=StrOutputParser()):\n",
    "\n",
    "  assistant = assistant_chain(system_message)\n",
    "  answer = assistant.invoke({\"question\": question})\n",
    "  print(answer)\n",
    "    \n",
    "  assert any(word in answer.lower() \\\n",
    "             for word in expected_words), \\\n",
    "    f\"Expected the assistant questions to include \\\n",
    "    '{expected_words}', but it did not\"\n",
    "\n",
    "def evaluate_refusal(\n",
    "    system_message,\n",
    "    question,\n",
    "    decline_response,\n",
    "    human_template=\"{question}\", \n",
    "    llm=ChatOpenAI(model=\"gpt-3.5-turbo\", temperature=0),\n",
    "    output_parser=StrOutputParser()):\n",
    "    \n",
    "  assistant = assistant_chain(human_template, \n",
    "                              system_message,\n",
    "                              llm,\n",
    "                              output_parser)\n",
    "  \n",
    "  answer = assistant.invoke({\"question\": question})\n",
    "  print(answer)\n",
    "  \n",
    "  assert decline_response.lower() in answer.lower(), \\\n",
    "    f\"Expected the bot to decline with \\\n",
    "    '{decline_response}' got {answer}\"\n",
    "\n",
    "\"\"\"\n",
    "  Test cases\n",
    "\"\"\"\n",
    "\n",
    "def test_science_quiz():\n",
    "  \n",
    "  question  = \"Generate a quiz about science.\"\n",
    "  expected_subjects = [\"davinci\", \"telescope\", \"physics\", \"curie\"]\n",
    "  eval_expected_words(\n",
    "      system_message,\n",
    "      question,\n",
    "      expected_subjects)\n",
    "\n",
    "def test_geography_quiz():\n",
    "  question  = \"Generate a quiz about geography.\"\n",
    "  expected_subjects = [\"paris\", \"france\", \"louvre\"]\n",
    "  eval_expected_words(\n",
    "      system_message,\n",
    "      question,\n",
    "      expected_subjects)\n",
    "\n",
    "def test_refusal_rome():\n",
    "  question  = \"Help me create a quiz about Rome\"\n",
    "  decline_response = \"I'm sorry\"\n",
    "  evaluate_refusal(\n",
    "      system_message,\n",
    "      question,\n",
    "      decline_response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Command to see the content of the file:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!cat test_assistant.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The CircleCI config file\n",
    "Now let's set up our tests to run automatically in CircleCI.\n",
    "\n",
    "For this course, we've created a working CircleCI config file. Let's take a look at the configuration.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!cat circle_config.yml"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run the per-commit evals\n",
    "Push files into the github repo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import push_files\n",
    "push_files(course_repo, course_branch, [\"app.py\", \"test_assistant.py\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Trigger the pipeline in CircleCI pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import trigger_commit_evals\n",
    "trigger_commit_evals(course_repo, course_branch, cci_api_key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (llmops)",
   "language": "python",
   "name": "llmops"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
